{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX.2 Klein 4B Character LoRA Pipeline\n",
    "\n",
    "mflux로 Mac에서 학습 → HuggingFace 업로드 → Runware AI 추론\n",
    "\n",
    "## Prerequisites\n",
    "- mflux >= 0.16.5 (`pip install mflux`)\n",
    "- `.env` file with HF_TOKEN, RUNWARE_API_KEY, HF_REPO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, shutil, zipfile, subprocess\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Project root\n",
    "ROOT = Path(os.getcwd()).parent if Path(os.getcwd()).name == \"notebooks\" else Path(os.getcwd())\n",
    "os.chdir(ROOT)\n",
    "sys.path.insert(0, str(ROOT / \"scripts\"))\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "RUNWARE_API_KEY = os.getenv(\"RUNWARE_API_KEY\")\n",
    "HF_REPO_ID = os.getenv(\"HF_REPO_ID\")\n",
    "\n",
    "assert HF_TOKEN, \"HF_TOKEN not set in .env\"\n",
    "assert RUNWARE_API_KEY, \"RUNWARE_API_KEY not set in .env\"\n",
    "assert HF_REPO_ID, \"HF_REPO_ID not set in .env\"\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"HF repo: {HF_REPO_ID}\")\n",
    "print(\"API keys loaded OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set your character name and trigger word here. All subsequent cells use these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EDIT THESE ===\n",
    "CHARACTER_NAME = \"my_character\"       # directory name, no spaces\n",
    "TRIGGER_WORD = \"ohwx_mychar\"          # unique trigger word\n",
    "LORA_VERSION = \"1\"                    # increment for new versions\n",
    "\n",
    "# === Derived paths (don't edit) ===\n",
    "RAW_DIR = ROOT / f\"datasets/{CHARACTER_NAME}/raw\"\n",
    "PROCESSED_DIR = ROOT / f\"datasets/{CHARACTER_NAME}/processed\"\n",
    "CAPTIONS_DIR = ROOT / f\"datasets/{CHARACTER_NAME}/captions\"\n",
    "MFLUX_DATA_DIR = ROOT / f\"datasets/{CHARACTER_NAME}/mflux\"\n",
    "TRAIN_CONFIG = ROOT / \"config/train.json\"\n",
    "CHECKPOINTS_DIR = ROOT / \"models/checkpoints\"\n",
    "\n",
    "for d in [RAW_DIR, PROCESSED_DIR, CAPTIONS_DIR, MFLUX_DATA_DIR, CHECKPOINTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Character: {CHARACTER_NAME}\")\n",
    "print(f\"Trigger: {TRIGGER_WORD}\")\n",
    "print(f\"Raw images go in: {RAW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "1. Place 15-25 raw images in the `raw/` directory shown above\n",
    "2. Run preprocessing (resize to 1024x1024 PNG)\n",
    "3. Run captioning (Florence-2 + trigger word)\n",
    "4. Merge into mflux data directory (image + txt pairs in same folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_images import preprocess_directory\n",
    "\n",
    "results = preprocess_directory(RAW_DIR, PROCESSED_DIR, size=1024, prefix=\"char\")\n",
    "print(f\"\\nProcessed {len(results)} images\")\n",
    "if not results:\n",
    "    print(\"\\u26a0 No images found. Place images in:\", RAW_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caption_images import caption_directory\n",
    "\n",
    "results = caption_directory(\n",
    "    PROCESSED_DIR,\n",
    "    CAPTIONS_DIR,\n",
    "    trigger_word=TRIGGER_WORD,\n",
    "    device=\"mps\",  # Apple Silicon\n",
    ")\n",
    "print(f\"\\nGenerated {len(results)} captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mflux expects image + txt pairs in the same directory\n",
    "# Copy processed images and captions into mflux data dir\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Clean previous data\n",
    "for f in MFLUX_DATA_DIR.iterdir():\n",
    "    f.unlink()\n",
    "\n",
    "count = 0\n",
    "for img_path in sorted(PROCESSED_DIR.glob(\"*.png\")):\n",
    "    txt_path = CAPTIONS_DIR / f\"{img_path.stem}.txt\"\n",
    "    if not txt_path.exists():\n",
    "        print(f\"WARNING: no caption for {img_path.name}, skipping\")\n",
    "        continue\n",
    "    shutil.copy2(img_path, MFLUX_DATA_DIR / img_path.name)\n",
    "    shutil.copy2(txt_path, MFLUX_DATA_DIR / txt_path.name)\n",
    "    count += 1\n",
    "\n",
    "# Add preview prompt for monitoring training progress\n",
    "preview_path = MFLUX_DATA_DIR / \"preview_1.txt\"\n",
    "preview_path.write_text(f\"{TRIGGER_WORD}, front view, neutral expression, white background, flat illustration style\")\n",
    "\n",
    "print(f\"Prepared {count} image+caption pairs in {MFLUX_DATA_DIR}\")\n",
    "print(f\"Preview prompt: {preview_path.read_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Runs mflux-train with the config template. Updates the data path to point to our character's mflux directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update train.json with current character's data path\n",
    "config = json.loads(TRAIN_CONFIG.read_text())\n",
    "config[\"data\"] = str(MFLUX_DATA_DIR) + \"/\"\n",
    "TRAIN_CONFIG.write_text(json.dumps(config, indent=2))\n",
    "\n",
    "print(f\"Training config updated: data = {config['data']}\")\n",
    "print(f\"Model: {config['model']}\")\n",
    "print(f\"Epochs: {config['training_loop']['num_epochs']}\")\n",
    "print(f\"Batch size: {config['training_loop']['batch_size']}\")\n",
    "print(f\"\\nStarting training... (this will take a while)\")\n",
    "\n",
    "# Run training\n",
    "result = subprocess.run(\n",
    "    [\"mflux-train\", \"--config\", str(TRAIN_CONFIG)],\n",
    "    cwd=str(ROOT),\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\nTraining complete!\")\n",
    "else:\n",
    "    print(f\"\\nTraining failed with exit code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract LoRA from Checkpoint\n",
    "\n",
    "mflux saves checkpoints as ZIP files. We extract the safetensors adapter from the best checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest training run\n",
    "train_dirs = sorted(ROOT.glob(\"train_*\"), key=lambda p: p.name)\n",
    "if not train_dirs:\n",
    "    raise FileNotFoundError(\"No training output found. Run training first.\")\n",
    "\n",
    "latest_run = train_dirs[-1]\n",
    "checkpoint_dir = latest_run / \"checkpoints\"\n",
    "checkpoints = sorted(checkpoint_dir.glob(\"*_checkpoint.zip\"))\n",
    "\n",
    "print(f\"Training run: {latest_run.name}\")\n",
    "print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "for cp in checkpoints:\n",
    "    print(f\"  {cp.name}\")\n",
    "\n",
    "# Use the latest checkpoint (highest step count)\n",
    "best_checkpoint = checkpoints[-1]\n",
    "print(f\"\\nUsing: {best_checkpoint.name}\")\n",
    "\n",
    "# Extract safetensors adapter\n",
    "output_safetensors = CHECKPOINTS_DIR / f\"{CHARACTER_NAME}.safetensors\"\n",
    "with zipfile.ZipFile(best_checkpoint) as zf:\n",
    "    adapter_files = [f for f in zf.namelist() if f.endswith(\"_adapter.safetensors\")]\n",
    "    if not adapter_files:\n",
    "        raise FileNotFoundError(f\"No adapter.safetensors found in {best_checkpoint.name}\")\n",
    "\n",
    "    adapter_name = adapter_files[0]\n",
    "    with zf.open(adapter_name) as src, open(output_safetensors, \"wb\") as dst:\n",
    "        dst.write(src.read())\n",
    "\n",
    "size_mb = output_safetensors.stat().st_size / (1024 * 1024)\n",
    "print(f\"\\nExtracted: {output_safetensors}\")\n",
    "print(f\"Size: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Local Test\n",
    "\n",
    "Quick inference with mflux-generate to verify the LoRA works before uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = f\"{TRIGGER_WORD}, front view, neutral expression, white background, flat illustration style, clean linework\"\n",
    "test_output = ROOT / \"validation/results\" / f\"{CHARACTER_NAME}_test.png\"\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"mflux-generate\",\n",
    "    \"--prompt\", test_prompt,\n",
    "    \"--model\", \"flux2-klein-base-4b\",\n",
    "    \"--steps\", \"20\",\n",
    "    \"--seed\", \"42\",\n",
    "    \"--lora-paths\", str(output_safetensors),\n",
    "    \"--lora-scales\", \"0.8\",\n",
    "    \"--output\", str(test_output),\n",
    "])\n",
    "\n",
    "if result.returncode == 0:\n",
    "    from PIL import Image\n",
    "    img = Image.open(test_output)\n",
    "    display(img)\n",
    "    print(f\"Saved: {test_output}\")\n",
    "else:\n",
    "    print(\"Generation failed. Check mflux installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload to HuggingFace Hub\n",
    "\n",
    "Uploads the safetensors file to a public HF repo so Runware can download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "# Create repo if it doesn't exist\n",
    "api.create_repo(repo_id=HF_REPO_ID, repo_type=\"model\", private=False, exist_ok=True)\n",
    "\n",
    "# Upload safetensors\n",
    "hf_filename = f\"{CHARACTER_NAME}.safetensors\"\n",
    "api.upload_file(\n",
    "    path_or_fileobj=str(output_safetensors),\n",
    "    path_in_repo=hf_filename,\n",
    "    repo_id=HF_REPO_ID,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "download_url = f\"https://huggingface.co/{HF_REPO_ID}/resolve/main/{hf_filename}\"\n",
    "print(f\"Uploaded: {hf_filename}\")\n",
    "print(f\"Download URL: {download_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Register on Runware AI\n",
    "\n",
    "Registers the LoRA with Runware so it can be used for inference via API.\n",
    "\n",
    "**Note:** The `architecture` field uses `flux1d` as FLUX.2-specific values are not yet in the SDK. If registration fails, contact Runware support for the correct architecture value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from runware import Runware, IUploadModelLora\n",
    "\n",
    "async def register_lora():\n",
    "    runware = Runware(api_key=RUNWARE_API_KEY)\n",
    "    await runware.connect()\n",
    "\n",
    "    payload = IUploadModelLora(\n",
    "        air=f\"civitai:{CHARACTER_NAME}@{LORA_VERSION}\",\n",
    "        name=f\"story-shorts-{CHARACTER_NAME}\",\n",
    "        downloadURL=download_url,\n",
    "        uniqueIdentifier=f\"story-shorts-{CHARACTER_NAME}-v{LORA_VERSION}\",\n",
    "        version=LORA_VERSION,\n",
    "        architecture=\"flux1d\",\n",
    "        format=\"safetensors\",\n",
    "        positiveTriggerWords=TRIGGER_WORD,\n",
    "        private=True,\n",
    "        shortDescription=f\"Character LoRA for {CHARACTER_NAME} (FLUX.2 Klein 4B)\",\n",
    "    )\n",
    "\n",
    "    result = await runware.modelUpload(payload)\n",
    "    return result\n",
    "\n",
    "upload_result = asyncio.run(register_lora())\n",
    "print(f\"Runware registration result: {upload_result}\")\n",
    "LORA_AIR = f\"civitai:{CHARACTER_NAME}@{LORA_VERSION}\"\n",
    "print(f\"LoRA AIR ID: {LORA_AIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Runware Inference Test\n",
    "\n",
    "Generate images using the deployed LoRA via Runware API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runware import Runware, IImageInference, ILora\n",
    "\n",
    "async def run_inference(prompt: str, lora_weight: float = 0.8):\n",
    "    runware = Runware(api_key=RUNWARE_API_KEY)\n",
    "    await runware.connect()\n",
    "\n",
    "    payload = IImageInference(\n",
    "        positivePrompt=prompt,\n",
    "        model=\"runware:400@5\",  # FLUX.2 Klein 4B Base\n",
    "        lora=[ILora(model=LORA_AIR, weight=lora_weight)],\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        numberResults=1,\n",
    "    )\n",
    "\n",
    "    images = await runware.imageInference(requestImage=payload)\n",
    "    return images\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    f\"{TRIGGER_WORD}, front view, neutral expression, white background, flat illustration\",\n",
    "    f\"{TRIGGER_WORD}, sitting and reading a book, cozy room, warm lighting, flat illustration\",\n",
    "    f\"{TRIGGER_WORD}, walking down a city street, daytime, flat style\",\n",
    "]\n",
    "\n",
    "from IPython.display import display, Image as IPImage\n",
    "import urllib.request\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n[{i+1}/{len(test_prompts)}] {prompt[:60]}...\")\n",
    "    images = asyncio.run(run_inference(prompt))\n",
    "    for img in images:\n",
    "        print(f\"  URL: {img.imageURL}\")\n",
    "        # Download and display\n",
    "        img_path = ROOT / f\"validation/results/{CHARACTER_NAME}_runware_{i+1}.png\"\n",
    "        urllib.request.urlretrieve(img.imageURL, str(img_path))\n",
    "        display(IPImage(filename=str(img_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}